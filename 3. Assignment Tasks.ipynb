{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f7cccff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_140849/3777615979.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14efb8a4",
   "metadata": {},
   "source": [
    "# (A) - Model Building and Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68fd5e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from lightfm import LightFM, evaluation, cross_validation\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66ce3558",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interaction_matrix(df, customer_col, product_col, purchase_column, norm = False):\n",
    "    '''\n",
    "    Function to create an interaction matrix dataframe in a format required by LightFM.\n",
    "    This function will take the below inputs and return a matrix of size M x N, where M \n",
    "    is the number of unique customers and N is the number of unique products.\n",
    "    \n",
    "    Inputs -\n",
    "        - df = Pandas DataFrame containing customer-product interactions (user-item)\n",
    "        - customer_col = column name containing customer's identifier\n",
    "        - product_col = column name containing product's identifier\n",
    "        - purchase_column col = column name containing customer purchase on interaction with a given product\n",
    "        - norm (optional) = True if a normalization of purchases are needed\n",
    "\n",
    "    Outputs - \n",
    "        - Pandas dataframe with customer-product interactions ready to be fed in a recommendation algorithm\n",
    "    '''\n",
    "    interactions = pd.pivot_table(df, values=purchase_column, index=customer_col, columns=product_col).fillna(0)\n",
    "    if norm:\n",
    "        interactions = (interactions-interactions.min())/(interactions.max()-interactions.min())\n",
    "\n",
    "    return interactions\n",
    "\n",
    "\n",
    "def create_customer_dict(interactions):\n",
    "    '''\n",
    "    Function to create a customer dictionary based on their \n",
    "    customer_ids and index position in interaction dataset\n",
    "    \n",
    "    Inputs - \n",
    "        interactions - dataset created by create_interaction_matrix() function\n",
    "        \n",
    "    Outputs -\n",
    "        customer_dict - Dictionary type output with, key = customer_ids, values =  corresponding index position in the interaction matrix\n",
    "    '''\n",
    "    customer_id = list(interactions.index)\n",
    "    customer_dict = {}\n",
    "    counter = 0 \n",
    "    for i in customer_id:\n",
    "        customer_dict[i] = counter\n",
    "        counter += 1\n",
    "    return customer_dict\n",
    "\n",
    "\n",
    "def create_product_dict(interactions):\n",
    "    '''\n",
    "    Function to create an product dictionary based on their product_id and product name\n",
    "    \n",
    "    Inputs - \n",
    "        interactions - dataset created by create_interaction_matrix() function\n",
    "        \n",
    "    Outputs -\n",
    "        product_dict = Dictionary type output with, key = product_ids, values =  corresponding index position in list(columns) in the interaction matrix\n",
    "    '''\n",
    "    product_id = list(interactions.columns)\n",
    "    product_dict = {}\n",
    "    counter = 0 \n",
    "    for i in product_id:\n",
    "        product_dict[i] = counter\n",
    "        counter += 1\n",
    "    return product_dict\n",
    "\n",
    "\n",
    "def train_MF_model(interactions, n_components=30, loss='warp', epoch=30, n_jobs = 4, learning_schedule = 'adadelta', learning_rate=0.0025):\n",
    "    '''\n",
    "    Function to run matrix-factorization algorithm on the prepared datasets.\n",
    "    Inputs -\n",
    "        - interactions = dataset create by create_interaction_matrix\n",
    "        - n_components = number of embeddings you want to create to define product and customer\n",
    "        - loss = loss function other options are logistic, brp\n",
    "        - epoch = number of epochs to run \n",
    "        - n_jobs = number of cores used for execution \n",
    "        \n",
    "    Outputs  -\n",
    "        Model - Trained model\n",
    "    '''\n",
    "\n",
    "    model = LightFM(no_components = n_components, \n",
    "                    learning_rate=0.0025, \n",
    "                    learning_schedule = learning_schedule,\n",
    "                    loss=loss,\n",
    "                    random_state = 42)\n",
    "    \n",
    "    model.fit(interactions, \n",
    "              epochs=epoch, \n",
    "              num_threads = n_jobs,\n",
    "              verbose = False)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def load_dataset(df_path, date_column):\n",
    "    \"\"\"\n",
    "    This function will be used to load the train and\n",
    "    test datasets.\n",
    "    \n",
    "    Inputs - \n",
    "        - df_path - Path of the dataset\n",
    "        - date_column - Column name of the date column for parsing\n",
    "    \n",
    "    Outputs -\n",
    "        - df - returns a dataframe object\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(df_path, parse_dates=[date_column])\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def sample_recommendation_of_products_to_customer(model, \n",
    "                                                  sparse_interactions, \n",
    "                                                  customer_id, \n",
    "                                                  customer_dict, \n",
    "                                                  X_train_interactions_columns, \n",
    "                                                  X_train_interactions_index,\n",
    "                                                  product_dict, \n",
    "                                                  threshold = 0, \n",
    "                                                  top_k_products_to_recommend = 3, \n",
    "                                                  top_k_purchased_products = 3):\n",
    "    '''\n",
    "    Function to produce top K customer recommendations\n",
    "    \n",
    "    Inputs - \n",
    "        - model = Trained matrix factorization model\n",
    "        - sparse_interactions = sparse dataset used for training the model\n",
    "        - X_train_interactions_columns - columns of the original non csr interaction df\n",
    "        - X_train_interactions_index - index of the original non csr interaction df\n",
    "        - customer_id = customer ID for which we need to generate recommendation\n",
    "        - customer_dict = Dictionary type input containing customer_ids as key and index positions as value\n",
    "        - product_dict = Dictionary type input containing product_id as key and corresponding index position in list(columns) of interaction_matrix as value\n",
    "        - threshold = value above which the purchase is favorable in new interaction matrix\n",
    "        - top_k_products_to_recommend = Number of output recommendation needed\n",
    "        - top_k_purchased_products = Number of top products the customaer has purchased\n",
    "        \n",
    "    Outputs - \n",
    "        - top_k_recommendations - returns the top k recommendations for a given customer\n",
    "        - top_products_purchased_before_by_this_customer - returns top k products purchased by the customer\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    interactions = pd.DataFrame.sparse.from_spmatrix(sparse_interactions, columns=X_train_interactions_columns)\n",
    "    interactions.index = X_train_interactions_index\n",
    "    interactions = interactions.sparse.to_dense()    \n",
    "    n_customers, n_products = interactions.shape\n",
    "    customer_index_in_dic = customer_dict[customer_id]\n",
    "    scores = pd.Series(model.predict(customer_index_in_dic,np.arange(n_products)))\n",
    "    scores.index = interactions.columns\n",
    "    scores = list(pd.Series(scores.sort_values(ascending=False).index))\n",
    "    known_products = list(pd.Series(interactions.loc[customer_id,:] [interactions.loc[customer_id,:] > threshold].index).sort_values(ascending=False))\n",
    "    scores = [x for x in scores if x not in known_products]\n",
    "    \n",
    "    top_k_recommendations = scores[0:top_k_products_to_recommend]\n",
    "    top_products_purchased_before_by_this_customer = known_products[:top_k_purchased_products]\n",
    "    \n",
    "    return top_k_recommendations, top_products_purchased_before_by_this_customer\n",
    "\n",
    "\n",
    "def create_customer_emdedding_distance_matrix(model, sparse_X_train_interactions, X_train_interactions_index, X_train_interactions_columns):\n",
    "    '''\n",
    "    Function to create customer-customer distance embedding matrix\n",
    "    \n",
    "    Inputs -\n",
    "        - model = Trained matrix factorization model\n",
    "        - sparse_X_train_interactions = dataset used for training the model\n",
    "        - X_train_interactions_index - index of X_train_interactions\n",
    "        - X_train_interactions_columns - columns of X_train_interactions\n",
    "        \n",
    "    Outputs -\n",
    "        - customer_emdedding_distance_matrix = Pandas dataframe containing cosine distance matrix b/w customers\n",
    "    '''\n",
    "    \n",
    "    interactions = pd.DataFrame.sparse.from_spmatrix(sparse_X_train_interactions, columns=X_train_interactions_columns)\n",
    "    interactions.index = X_train_interactions_index\n",
    "    interactions = interactions.sparse.to_dense() \n",
    "    df_customer_norm_sparse = sparse.csr_matrix(model.user_embeddings)\n",
    "    similarities = cosine_similarity(df_customer_norm_sparse)\n",
    "    customer_emdedding_distance_matrix = pd.DataFrame(similarities)\n",
    "    customer_emdedding_distance_matrix.columns = interactions.index\n",
    "    customer_emdedding_distance_matrix.index = interactions.index\n",
    "    return customer_emdedding_distance_matrix\n",
    "\n",
    "\n",
    "def customer_customer_recommendation(customer_emdedding_distance_matrix, customer_id, customer_dict, n_customers = 5, show = True):\n",
    "    '''\n",
    "    Function to create customer-customer recommendation\n",
    "    \n",
    "    Inputs - \n",
    "        - customer_emdedding_distance_matrix = Pandas dataframe containing cosine distance matrix b/w customers\n",
    "        - customer_id  = customer ID for which we need to generate recommended customers\n",
    "        - customer_dict = Dictionary type input containing customer_id as key and customer_name as value\n",
    "        - n_customers = Number of customers needed as an output\n",
    "    Outputs -\n",
    "        - recommended_customers = List of recommended customers\n",
    "    '''\n",
    "    recommended_customers = list(pd.Series(customer_emdedding_distance_matrix.loc[customer_id,:].sort_values(ascending = False).head(n_customers+1).index[1:n_customers+1]))\n",
    "\n",
    "    print(\"Top {} customer similar to the above customer: \".format(n_customers))\n",
    "    print(recommended_customers)\n",
    "    \n",
    "    return recommended_customers\n",
    "\n",
    "\n",
    "def return_common_products_in_train_and_holdout(X_train, X_holdout):\n",
    "    '''\n",
    "    Function to find out common product ids present across X_train\n",
    "    and X_holdout datasets. This is done to create same features dimensions\n",
    "    across both the matrices, otherwise I was not able to evaluate the\n",
    "    holdout datasets.\n",
    "    \n",
    "    Inputs - \n",
    "        - X_train = Given dataset for training (in pandas DataFrame format)\n",
    "        - X_holdout = Given dataset for holdout (in pandas DataFrame format)\n",
    "\n",
    "    Outputs -\n",
    "        - common_items = List of common products present in both the datasets\n",
    "    '''\n",
    "    items_train = list(X_train['Product_num'].unique())\n",
    "    items_holdout = list(X_holdout['Product_num'].unique())\n",
    "    common_items = list(set(items_train).intersection(set(items_holdout)))\n",
    "    \n",
    "    return common_items    \n",
    "\n",
    "def evaluate_model(model, data, eval_stage = 'Training'):\n",
    "    '''\n",
    "    Function used for evaluating model performances on train set\n",
    "    and cross validation dataset.\n",
    "    \n",
    "    Required Input - \n",
    "        - data = data should be in CSR format\n",
    "        - eval_stage  = Which phase of experiment is this? Train, cross validation, holdout etc.\n",
    "        - model = Dictionary type input containing customer_id as key and customer_name as value\n",
    "\n",
    "    Expected Output -\n",
    "        - prints the AUC, Recall@K and Precisio@K values\n",
    "    '''\n",
    "    \n",
    "    print(\"{} AUC: \".format(eval_stage), evaluation.auc_score(model, data, num_threads=8).mean())\n",
    "    print(\"{} Recall@K: \".format(eval_stage), evaluation.recall_at_k(model, data, k=3, num_threads=8).mean())\n",
    "    print(\"{} Precision@K: \".format(eval_stage), evaluation.precision_at_k(model, data, k=3, num_threads=8).mean())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11aeee52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test_holdout is a holdout dataset, shall be only used for final evaluation and should not be part of the training methodologies\n",
    "train_df_path = 'data/prepared_datasets_for_training_and_evaluation/reco_assignment_training_merged_duplicates.csv'\n",
    "date_column = 'Tran_dt'\n",
    "X_train = load_dataset(train_df_path, date_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bbc4cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating interaction matrix for X_train data\n",
    "X_train_interactions = create_interaction_matrix(df = X_train,\n",
    "                                         customer_col = 'Customer_num',\n",
    "                                         product_col = 'Product_num',\n",
    "                                         purchase_column = 'Total_Tran_qty')\n",
    "\n",
    "X_train_interactions_columns = list(X_train_interactions.columns)\n",
    "X_train_interactions_index = list(X_train_interactions.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c5f1b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_interactions:  (9835, 17411)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Product_num</th>\n",
       "      <th>P_10</th>\n",
       "      <th>P_1000</th>\n",
       "      <th>P_10002</th>\n",
       "      <th>P_10007</th>\n",
       "      <th>P_10008</th>\n",
       "      <th>P_10009</th>\n",
       "      <th>P_1001</th>\n",
       "      <th>P_10010</th>\n",
       "      <th>P_10011</th>\n",
       "      <th>P_10012</th>\n",
       "      <th>...</th>\n",
       "      <th>P_9978</th>\n",
       "      <th>P_9979</th>\n",
       "      <th>P_998</th>\n",
       "      <th>P_9980</th>\n",
       "      <th>P_9983</th>\n",
       "      <th>P_9984</th>\n",
       "      <th>P_9988</th>\n",
       "      <th>P_999</th>\n",
       "      <th>P_9991</th>\n",
       "      <th>P_9992</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Customer_num</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>C_100082</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_1001004</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_1001197</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_1001322</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_1001329</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 17411 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Product_num   P_10  P_1000  P_10002  P_10007  P_10008  P_10009  P_1001  \\\n",
       "Customer_num                                                             \n",
       "C_100082       0.0     0.0      0.0      0.0      0.0      0.0     0.0   \n",
       "C_1001004      0.0     0.0      0.0      0.0      0.0      0.0     0.0   \n",
       "C_1001197      0.0     0.0      0.0      0.0      0.0      0.0     0.0   \n",
       "C_1001322      0.0     0.0      0.0      0.0      0.0      0.0     0.0   \n",
       "C_1001329      0.0     0.0      0.0      0.0      0.0      0.0     0.0   \n",
       "\n",
       "Product_num   P_10010  P_10011  P_10012  ...  P_9978  P_9979  P_998  P_9980  \\\n",
       "Customer_num                             ...                                  \n",
       "C_100082          0.0      0.0      0.0  ...     0.0     0.0    0.0     0.0   \n",
       "C_1001004         0.0      0.0      0.0  ...     0.0     0.0    0.0     0.0   \n",
       "C_1001197         0.0      0.0      0.0  ...     0.0     0.0    0.0     0.0   \n",
       "C_1001322         0.0      0.0      0.0  ...     0.0     0.0    0.0     0.0   \n",
       "C_1001329         0.0      0.0      0.0  ...     0.0     0.0    0.0     0.0   \n",
       "\n",
       "Product_num   P_9983  P_9984  P_9988  P_999  P_9991  P_9992  \n",
       "Customer_num                                                 \n",
       "C_100082         0.0     0.0     0.0    0.0     0.0     0.0  \n",
       "C_1001004        0.0     0.0     0.0    0.0     0.0     0.0  \n",
       "C_1001197        0.0     0.0     0.0    0.0     0.0     0.0  \n",
       "C_1001322        0.0     0.0     0.0    0.0     0.0     0.0  \n",
       "C_1001329        0.0     0.0     0.0    0.0     0.0     0.0  \n",
       "\n",
       "[5 rows x 17411 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Shape of X_train_interactions: \", X_train_interactions.shape)\n",
    "X_train_interactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40d1836a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Train customer Dict\n",
    "customer_dict_train = create_customer_dict(interactions=X_train_interactions)\n",
    "\n",
    "# Create Train product Dict\n",
    "product_dict_train = create_product_dict(interactions=X_train_interactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db88741f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(customer_dict_train, open('data/interaction_matrices/customer_dict_train.pkl','wb'))\n",
    "pkl.dump(product_dict_train, open('data/interaction_matrices/product_dict_train.pkl','wb'))\n",
    "pkl.dump(X_train_interactions_columns, open('data/interaction_matrices/X_train_interactions_columns.pkl','wb'))\n",
    "pkl.dump(X_train_interactions_index, open('data/interaction_matrices/X_train_interactions_index.pkl','wb'))\n",
    "sparse.save_npz(\"data/interaction_matrices/sparse_X_train_interactions.npz\", sparse.csr_matrix(X_train_interactions.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "397ad98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data matrix:  (9835, 17411)\n",
      "Shape of cross validation data matrix:  (9835, 17411)\n"
     ]
    }
   ],
   "source": [
    "#Create train and validation COO matrices from X_train_interactions, such that the train and validation coo matrices shapes are equal, i.e. (9835, 13082)\n",
    "data_train, data_val = cross_validation.random_train_test_split(sparse.csr_matrix(X_train_interactions.values), test_percentage=0.2, random_state=None)\n",
    "\n",
    "print(\"Shape of training data matrix: \", data_train.shape)\n",
    "print(\"Shape of cross validation data matrix: \", data_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4876b28",
   "metadata": {},
   "source": [
    "## Experiments with different models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41289c1",
   "metadata": {},
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21cdc7a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AUC:  0.9995824\n",
      "Training Recall@K:  0.17852685537424398\n",
      "Training Precision@K:  0.6455358\n",
      "\n",
      "Cross Validation AUC:  0.7101558\n",
      "Cross Validation Recall@K:  0.0074578005403751115\n",
      "Cross Validation Precision@K:  0.008595876\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Training the recommendation model\n",
    "model_1 = train_MF_model(data_train, n_components=100, loss='warp', \n",
    "                                epoch=100, n_jobs = 8, learning_schedule = 'adadelta', learning_rate=0.0025)\n",
    "\n",
    "#Evaluation on Training Data\n",
    "evaluate_model(model_1, data_train, eval_stage = 'Training')\n",
    "\n",
    "#Evaluation on Cross Validation Data\n",
    "evaluate_model(model_1, data_val, eval_stage = 'Cross Validation')\n",
    "\n",
    "#Save model to disk for offline usage\n",
    "pkl.dump(model_1, open('models/model_1.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1790f499",
   "metadata": {},
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55ab31c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AUC:  0.99993265\n",
      "Training Recall@K:  0.29730100909425083\n",
      "Training Precision@K:  0.8859892\n",
      "\n",
      "Cross Validation AUC:  0.6813255\n",
      "Cross Validation Recall@K:  0.00208377488810257\n",
      "Cross Validation Precision@K:  0.0021392454\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Training the recommendation model\n",
    "model_2 = train_MF_model(data_train, n_components=100, loss='warp', \n",
    "                                epoch=500, n_jobs = 8, learning_schedule = 'adadelta', learning_rate=0.0025)\n",
    "\n",
    "#Evaluation on Training Data\n",
    "evaluate_model(model_2, data_train, eval_stage = 'Training')\n",
    "\n",
    "#Evaluation on Cross Validation Data\n",
    "evaluate_model(model_2, data_val, eval_stage = 'Cross Validation')\n",
    "\n",
    "#Save model to disk for offline usage\n",
    "pkl.dump(model_2, open('models/model_2.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db733567",
   "metadata": {},
   "source": [
    "### Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7d92011",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AUC:  0.99998754\n",
      "Training Recall@K:  0.32349274027823655\n",
      "Training Precision@K:  0.94191575\n",
      "\n",
      "Cross Validation AUC:  0.66376823\n",
      "Cross Validation Recall@K:  0.0009710452412422349\n",
      "Cross Validation Precision@K:  0.0007779075\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Training the recommendation model\n",
    "model_3 = train_MF_model(data_train, n_components=100, loss='warp', \n",
    "                                epoch=1000, n_jobs = 8, learning_schedule = 'adadelta', learning_rate=0.0025)\n",
    "\n",
    "#Evaluation on Training Data\n",
    "evaluate_model(model_3, data_train, eval_stage = 'Training')\n",
    "\n",
    "#Evaluation on Cross Validation Data\n",
    "evaluate_model(model_3, data_val, eval_stage = 'Cross Validation')\n",
    "\n",
    "#Save model to disk for offline usage\n",
    "pkl.dump(model_3, open('models/model_3.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf43e16",
   "metadata": {},
   "source": [
    "### Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ced97e47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AUC:  0.86448205\n",
      "Training Recall@K:  0.013351382661447683\n",
      "Training Precision@K:  0.050926402\n",
      "\n",
      "Cross Validation AUC:  0.8254915\n",
      "Cross Validation Recall@K:  0.008848116832992147\n",
      "Cross Validation Precision@K:  0.013691171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Training the recommendation model\n",
    "model_4 = train_MF_model(data_train, n_components=100, loss='logistic', \n",
    "                                epoch=1000, n_jobs = 8, learning_schedule = 'adadelta', learning_rate=0.0025)\n",
    "\n",
    "#Evaluation on Training Data\n",
    "evaluate_model(model_4, data_train, eval_stage = 'Training')\n",
    "\n",
    "#Evaluation on Cross Validation Data\n",
    "evaluate_model(model_4, data_val, eval_stage = 'Cross Validation')\n",
    "\n",
    "#Save model to disk for offline usage\n",
    "pkl.dump(model_4, open('models/model_4.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b8990b",
   "metadata": {},
   "source": [
    "### Model 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80008dee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AUC:  0.8827915\n",
      "Training Recall@K:  0.021307612412536127\n",
      "Training Precision@K:  0.14349806\n",
      "\n",
      "Cross Validation AUC:  0.8412493\n",
      "Cross Validation Recall@K:  0.021200752532024802\n",
      "Cross Validation Precision@K:  0.036367174\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Training the recommendation model\n",
    "model_8 = train_MF_model(data_train, n_components=100, loss='warp', \n",
    "                                epoch=1000, n_jobs = 8, learning_schedule = 'adagrad', learning_rate=0.0025)\n",
    "\n",
    "#Evaluation on Training Data\n",
    "evaluate_model(model_8, data_train, eval_stage = 'Training')\n",
    "\n",
    "#Evaluation on Cross Validation Data\n",
    "evaluate_model(model_8, data_val, eval_stage = 'Cross Validation')\n",
    "\n",
    "#Save model to disk for offline usage\n",
    "pkl.dump(model_8, open('models/model_8.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c824fb",
   "metadata": {},
   "source": [
    "### Model 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6577269",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AUC:  0.97013414\n",
      "Training Recall@K:  0.32786652633241364\n",
      "Training Precision@K:  0.95516974\n",
      "\n",
      "Cross Validation AUC:  0.6499689\n",
      "Cross Validation Recall@K:  0.0007130818099312848\n",
      "Cross Validation Precision@K:  0.00035005834\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Training the recommendation model\n",
    "model_6 = train_MF_model(data_train, n_components=100, loss='warp-kos', \n",
    "                                epoch=1000, n_jobs = 8, learning_schedule = 'adadelta', learning_rate=0.0025)\n",
    "\n",
    "#Evaluation on Training Data\n",
    "evaluate_model(model_6, data_train, eval_stage = 'Training')\n",
    "\n",
    "#Evaluation on Cross Validation Data\n",
    "evaluate_model(model_6, data_val, eval_stage = 'Cross Validation')\n",
    "\n",
    "#Save model to disk for offline usage\n",
    "pkl.dump(model_6, open('models/model_6.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c6bce5",
   "metadata": {},
   "source": [
    "### Model 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2fbf8e7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AUC:  0.8530314\n",
      "Training Recall@K:  0.0180677619825127\n",
      "Training Precision@K:  0.113531284\n",
      "\n",
      "Cross Validation AUC:  0.8377974\n",
      "Cross Validation Recall@K:  0.019367296423577318\n",
      "Cross Validation Precision@K:  0.032244265\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Training the recommendation model\n",
    "model_7 = train_MF_model(data_train, n_components=100, loss='logistic', \n",
    "                                epoch=1000, n_jobs = 8, learning_schedule = 'adagrad', learning_rate=0.0025)\n",
    "\n",
    "#Evaluation on Training Data\n",
    "evaluate_model(model_7, data_train, eval_stage = 'Training')\n",
    "\n",
    "#Evaluation on Cross Validation Data\n",
    "evaluate_model(model_7, data_val, eval_stage = 'Cross Validation')\n",
    "\n",
    "#Save model to disk for offline usage\n",
    "pkl.dump(model_7, open('models/model_7.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eea920b",
   "metadata": {},
   "source": [
    "### Model 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69029d65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AUC:  0.9998785\n",
      "Training Recall@K:  0.3289971369604419\n",
      "Training Precision@K:  0.90068644\n",
      "\n",
      "Cross Validation AUC:  0.5312512\n",
      "Cross Validation Recall@K:  0.0006436677114821878\n",
      "Cross Validation Precision@K:  0.0007027407\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Training the recommendation model\n",
    "model_5 = train_MF_model(data_train, n_components=100, loss='bpr', \n",
    "                                epoch=1000, n_jobs = 8, learning_schedule = 'adadelta', learning_rate=0.0025)\n",
    "\n",
    "#Evaluation on Training Data\n",
    "evaluate_model(model_5, data_train, eval_stage = 'Training')\n",
    "\n",
    "#Evaluation on Cross Validation Data\n",
    "evaluate_model(model_5, data_val, eval_stage = 'Cross Validation')\n",
    "\n",
    "#Save model to disk for offline usage\n",
    "pkl.dump(model_5, open('models/model_5.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39ea99c",
   "metadata": {},
   "source": [
    "# (B) - Final Model Selection based on Cross Validation AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0263a1d6",
   "metadata": {},
   "source": [
    "\n",
    "From the above experiments, I have selected the model 5 which is giving highest cross validation AUC. Now, we will use the whole train and cross validation data to build our final model, and set the hyperparameters value to the value of the best model obtained from above. We will basically refit the model with the whole training dataset that is available. We will use this model for completing our assignment task. \n",
    "\n",
    "NOTE: I did not use RandomizedSearchCV for hyperparamter tuning due to time constraints. I am doing the hyperparameter tuning manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79db7acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AUC:  0.87198097\n",
      "Training Recall@K:  0.021025623321163815\n",
      "Training Precision@K:  0.16790375\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparse_X_train_interactions = sparse.load_npz('data/interaction_matrices/sparse_X_train_interactions.npz')\n",
    "\n",
    "#Training the recommendation model on the whole dataset to create the final model (Based on Model 5's hyper-parameters)\n",
    "final_trained_model = train_MF_model(sparse_X_train_interactions, n_components=100, loss='warp', \n",
    "                                epoch=1000, n_jobs = 8, learning_schedule = 'adagrad', learning_rate=0.0025)\n",
    "\n",
    "#Evaluation on Training Data\n",
    "evaluate_model(final_trained_model, sparse_X_train_interactions, eval_stage = 'Training')\n",
    "\n",
    "#Save model to disk for offline usage\n",
    "pkl.dump(final_trained_model, open('models/final_trained_model.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1198c7",
   "metadata": {},
   "source": [
    "# (C) -  Assignment Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417bd229",
   "metadata": {},
   "source": [
    "## Load all the artefacts related to the final trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8fc4b6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_dict_train = pkl.load(open('data/interaction_matrices/customer_dict_train.pkl','rb'))\n",
    "product_dict_train = pkl.load(open('data/interaction_matrices/product_dict_train.pkl','rb'))\n",
    "X_train_interactions_columns = pkl.load(open('data/interaction_matrices/X_train_interactions_columns.pkl','rb'))\n",
    "X_train_interactions_index = pkl.load(open('data/interaction_matrices/X_train_interactions_index.pkl','rb'))\n",
    "final_trained_model = pkl.load(open('models/final_trained_model.pkl','rb'))\n",
    "sparse_X_train_interactions = sparse.load_npz('data/interaction_matrices/sparse_X_train_interactions.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddd3bbb",
   "metadata": {},
   "source": [
    "## 1. What are the next 3 products to be purchased by a customer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57ad5512",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 products recommendations for customer id C_1005061:  ['P_16717', 'P_7819', 'P_12777']\n",
      "Top 3 most known products purchased by customer-id C_1005061 before:  ['P_9590', 'P_9300', 'P_9160']\n"
     ]
    }
   ],
   "source": [
    "def recommend_products(model, interactions, customer_id, \n",
    "                       customer_dict, product_dict, \n",
    "                       X_train_interactions_columns, X_train_interactions_index,\n",
    "                       top_k_products_to_recommend=3, \n",
    "                       top_k_purchased_products=3):\n",
    "    \n",
    "    top_k_recommendations, top_products_purchased_before_by_this_customer = sample_recommendation_of_products_to_customer(model = model, \n",
    "                                                                                                                        sparse_interactions = interactions, \n",
    "                                                                                                                        customer_id = customer_id, \n",
    "                                                                                                                        customer_dict = customer_dict,\n",
    "                                                                                                                        product_dict = product_dict, \n",
    "                                                                                                                        X_train_interactions_columns = X_train_interactions_columns,\n",
    "                                                                                                                        X_train_interactions_index = X_train_interactions_index,\n",
    "                                                                                                                        threshold = 0,\n",
    "                                                                                                                        top_k_products_to_recommend = top_k_products_to_recommend,\n",
    "                                                                                                                        top_k_purchased_products = top_k_purchased_products)\n",
    "    \n",
    "    print(\"Top {} products recommendations for customer id {}: \".format(top_k_products_to_recommend, customer_id), top_k_recommendations)\n",
    "    print(\"Top {} most known products purchased by customer-id {} before: \".format(top_k_purchased_products, customer_id), top_products_purchased_before_by_this_customer)\n",
    "\n",
    "#Randmoly pick up a customer ID from the holdout dataset\n",
    "#customer_id = X_train_interactions['Customer_num'].sample(1).values[0]\n",
    "customer_id = 'C_1005061'\n",
    "model = final_trained_model\n",
    "interactions = sparse_X_train_interactions\n",
    "customer_dict = customer_dict_train\n",
    "product_dict = product_dict_train\n",
    "top_k_products_to_recommend = 3\n",
    "top_k_purchased_products = 3\n",
    "\n",
    "recommend_products(model = model, \n",
    "                    interactions = interactions, \n",
    "                    customer_id = customer_id, \n",
    "                    customer_dict = customer_dict,\n",
    "                    product_dict = product_dict, \n",
    "                    X_train_interactions_columns = X_train_interactions_columns,\n",
    "                    X_train_interactions_index = X_train_interactions_index,\n",
    "                    top_k_products_to_recommend = top_k_products_to_recommend,\n",
    "                    top_k_purchased_products = top_k_purchased_products)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61994ff0",
   "metadata": {},
   "source": [
    "## 2. Who are the top 5 similar customers to each customer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d34dd15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 customer similar to the above customer: \n",
      "['C_2173177', 'C_2209071', 'C_15982', 'C_751312', 'C_807916']\n"
     ]
    }
   ],
   "source": [
    "customer_id = 'C_516469'\n",
    "customer_emdedding_distance_matrix = create_customer_emdedding_distance_matrix(model, sparse_X_train_interactions, X_train_interactions_index, X_train_interactions_columns)\n",
    "recommended_customers = customer_customer_recommendation(customer_emdedding_distance_matrix, customer_id, customer_dict_train, n_customers = 5, show = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b98385a",
   "metadata": {},
   "source": [
    "## 3. Recall calculation on the basis of the shared holdout set.\n",
    "\n",
    "NOTE : The shape of the interaction matrix of the training dataset is (9835, 17411), and the shape of the interaction matrix of the holdout dataset is (9835, 16230). For evaluation in LightFM, the above two shapes should be same. Hence I added another dummy matrix filled with zeroes to make the final shape of the holdout interaction matrix to be (9835, 17411). The dummy variables in the holdout dataset, can be intuitively thought of as new products being added, but no customer has purchased that product till now, hence the customer-product interaction will be zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "efd336e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9835, 16230)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X_test_holdout is a holdout dataset, shall be only used for final evaluation and should not be part of the training methodologies\n",
    "holdout_df_path = 'data/prepared_datasets_for_training_and_evaluation/reco_assignment_holdout_merged_duplicates.csv'\n",
    "date_column = 'Tran_dt'\n",
    "X_holdout = load_dataset(holdout_df_path, date_column)\n",
    "\n",
    "# Creating interaction matrix for X_holdout data\n",
    "X_holdout_interactions = create_interaction_matrix(df = X_holdout,\n",
    "                                                    customer_col = 'Customer_num',\n",
    "                                                    product_col = 'Product_num',\n",
    "                                                    purchase_column = 'Total_Tran_qty')\n",
    "\n",
    "X_holdout_interactions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "890a9769",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_train = X_train_interactions_columns\n",
    "items_holdout = list(X_holdout_interactions.columns)\n",
    "\n",
    "N = len(items_train) - len(items_holdout) #num_of_dummy_columns_to_add_to_holdout\n",
    "M = X_holdout_interactions.shape[0]\n",
    "\n",
    "dummy_columns = [\"Dummy_{}\".format(i) for i in range(1, N+1)]\n",
    "\n",
    "zero_data = np.zeros(shape=(M,N))\n",
    "dummy = pd.DataFrame(zero_data, columns=dummy_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0361ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_holdout_interactions_modified:  (9835, 17411)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P_10</th>\n",
       "      <th>P_1000</th>\n",
       "      <th>P_10001</th>\n",
       "      <th>P_10002</th>\n",
       "      <th>P_10007</th>\n",
       "      <th>P_10010</th>\n",
       "      <th>P_10011</th>\n",
       "      <th>P_10012</th>\n",
       "      <th>P_10013</th>\n",
       "      <th>P_10014</th>\n",
       "      <th>...</th>\n",
       "      <th>Dummy_1172</th>\n",
       "      <th>Dummy_1173</th>\n",
       "      <th>Dummy_1174</th>\n",
       "      <th>Dummy_1175</th>\n",
       "      <th>Dummy_1176</th>\n",
       "      <th>Dummy_1177</th>\n",
       "      <th>Dummy_1178</th>\n",
       "      <th>Dummy_1179</th>\n",
       "      <th>Dummy_1180</th>\n",
       "      <th>Dummy_1181</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Customer_num</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>C_100082</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_1001004</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_1001197</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_1001322</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_1001329</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 17411 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              P_10  P_1000  P_10001  P_10002  P_10007  P_10010  P_10011  \\\n",
       "Customer_num                                                              \n",
       "C_100082       0.0     0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "C_1001004      0.0     0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "C_1001197      0.0     0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "C_1001322      0.0     0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "C_1001329      0.0     0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "              P_10012  P_10013  P_10014  ...  Dummy_1172  Dummy_1173  \\\n",
       "Customer_num                             ...                           \n",
       "C_100082          0.0      0.0      0.0  ...         0.0         0.0   \n",
       "C_1001004         0.0      0.0      0.0  ...         0.0         0.0   \n",
       "C_1001197         0.0      0.0      0.0  ...         0.0         0.0   \n",
       "C_1001322         0.0      0.0      0.0  ...         0.0         0.0   \n",
       "C_1001329         0.0      0.0      0.0  ...         0.0         0.0   \n",
       "\n",
       "              Dummy_1174  Dummy_1175  Dummy_1176  Dummy_1177  Dummy_1178  \\\n",
       "Customer_num                                                               \n",
       "C_100082             0.0         0.0         0.0         0.0         0.0   \n",
       "C_1001004            0.0         0.0         0.0         0.0         0.0   \n",
       "C_1001197            0.0         0.0         0.0         0.0         0.0   \n",
       "C_1001322            0.0         0.0         0.0         0.0         0.0   \n",
       "C_1001329            0.0         0.0         0.0         0.0         0.0   \n",
       "\n",
       "              Dummy_1179  Dummy_1180  Dummy_1181  \n",
       "Customer_num                                      \n",
       "C_100082             0.0         0.0         0.0  \n",
       "C_1001004            0.0         0.0         0.0  \n",
       "C_1001197            0.0         0.0         0.0  \n",
       "C_1001322            0.0         0.0         0.0  \n",
       "C_1001329            0.0         0.0         0.0  \n",
       "\n",
       "[5 rows x 17411 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = X_holdout_interactions.values\n",
    "B = dummy.values\n",
    "\n",
    "C = np.hstack([A,B])\n",
    "\n",
    "X_holdout_interactions_modified = pd.DataFrame(C)\n",
    "X_holdout_interactions_modified.columns = list(X_holdout_interactions.columns) + dummy_columns\n",
    "X_holdout_interactions_modified.index = X_holdout_interactions.index\n",
    "print(\"Shape of X_holdout_interactions_modified: \",X_holdout_interactions_modified.shape)\n",
    "X_holdout_interactions_modified.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05dc53cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holdout Dataset AUC:  0.5201246\n",
      "Holdout Dataset Recall@K:  0.00014755065136246246\n",
      "Holdout Dataset Precision@K:  0.001084562\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#AUC and Recall@K of the final trained model on the holdout dataset\n",
    "evaluate_model(final_trained_model, sparse.csr_matrix(X_holdout_interactions_modified.values), eval_stage = 'Holdout Dataset')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
